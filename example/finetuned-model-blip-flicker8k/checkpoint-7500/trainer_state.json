{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 200,
  "global_step": 7500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 9.322359085083008,
      "learning_rate": 4.967333333333334e-05,
      "loss": 5.0906,
      "step": 50
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 10.837078094482422,
      "learning_rate": 4.9340000000000005e-05,
      "loss": 4.446,
      "step": 100
    },
    {
      "epoch": 0.1,
      "grad_norm": 11.689992904663086,
      "learning_rate": 4.9006666666666666e-05,
      "loss": 4.2472,
      "step": 150
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 9.325711250305176,
      "learning_rate": 4.867333333333334e-05,
      "loss": 4.3041,
      "step": 200
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 10.138952255249023,
      "learning_rate": 4.834e-05,
      "loss": 4.2289,
      "step": 250
    },
    {
      "epoch": 0.2,
      "grad_norm": 10.487863540649414,
      "learning_rate": 4.800666666666667e-05,
      "loss": 4.1406,
      "step": 300
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 10.097503662109375,
      "learning_rate": 4.7673333333333337e-05,
      "loss": 4.1643,
      "step": 350
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 10.500694274902344,
      "learning_rate": 4.7340000000000004e-05,
      "loss": 4.2331,
      "step": 400
    },
    {
      "epoch": 0.3,
      "grad_norm": 8.449275016784668,
      "learning_rate": 4.700666666666667e-05,
      "loss": 4.1959,
      "step": 450
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 10.985074043273926,
      "learning_rate": 4.667333333333333e-05,
      "loss": 4.0971,
      "step": 500
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 9.658941268920898,
      "learning_rate": 4.634e-05,
      "loss": 4.1894,
      "step": 550
    },
    {
      "epoch": 0.4,
      "grad_norm": 10.704721450805664,
      "learning_rate": 4.600666666666667e-05,
      "loss": 4.117,
      "step": 600
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 11.154580116271973,
      "learning_rate": 4.5673333333333335e-05,
      "loss": 4.0142,
      "step": 650
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 8.413297653198242,
      "learning_rate": 4.534e-05,
      "loss": 4.2622,
      "step": 700
    },
    {
      "epoch": 0.5,
      "grad_norm": 10.869611740112305,
      "learning_rate": 4.500666666666667e-05,
      "loss": 4.1268,
      "step": 750
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 8.90600872039795,
      "learning_rate": 4.467333333333333e-05,
      "loss": 4.0588,
      "step": 800
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 9.387531280517578,
      "learning_rate": 4.4340000000000006e-05,
      "loss": 4.1416,
      "step": 850
    },
    {
      "epoch": 0.6,
      "grad_norm": 9.990891456604004,
      "learning_rate": 4.4006666666666667e-05,
      "loss": 3.9017,
      "step": 900
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 10.526806831359863,
      "learning_rate": 4.3673333333333334e-05,
      "loss": 4.0525,
      "step": 950
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 9.463329315185547,
      "learning_rate": 4.334e-05,
      "loss": 4.0192,
      "step": 1000
    },
    {
      "epoch": 0.7,
      "grad_norm": 9.596506118774414,
      "learning_rate": 4.300666666666667e-05,
      "loss": 3.9307,
      "step": 1050
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 10.616537094116211,
      "learning_rate": 4.267333333333334e-05,
      "loss": 3.9701,
      "step": 1100
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 8.818422317504883,
      "learning_rate": 4.2340000000000005e-05,
      "loss": 4.0659,
      "step": 1150
    },
    {
      "epoch": 0.8,
      "grad_norm": 9.841992378234863,
      "learning_rate": 4.2006666666666665e-05,
      "loss": 4.0073,
      "step": 1200
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 9.95671558380127,
      "learning_rate": 4.167333333333334e-05,
      "loss": 3.9184,
      "step": 1250
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 11.257620811462402,
      "learning_rate": 4.134e-05,
      "loss": 4.0189,
      "step": 1300
    },
    {
      "epoch": 0.9,
      "grad_norm": 9.362482070922852,
      "learning_rate": 4.100666666666667e-05,
      "loss": 4.0359,
      "step": 1350
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 9.123930931091309,
      "learning_rate": 4.0673333333333336e-05,
      "loss": 3.9742,
      "step": 1400
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 8.423989295959473,
      "learning_rate": 4.034e-05,
      "loss": 3.9016,
      "step": 1450
    },
    {
      "epoch": 1.0,
      "grad_norm": 9.800586700439453,
      "learning_rate": 4.000666666666667e-05,
      "loss": 3.9093,
      "step": 1500
    },
    {
      "epoch": 1.0333333333333334,
      "grad_norm": 8.529732704162598,
      "learning_rate": 3.967333333333333e-05,
      "loss": 3.8441,
      "step": 1550
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 10.011189460754395,
      "learning_rate": 3.9340000000000006e-05,
      "loss": 3.8327,
      "step": 1600
    },
    {
      "epoch": 1.1,
      "grad_norm": 10.124678611755371,
      "learning_rate": 3.900666666666667e-05,
      "loss": 3.8862,
      "step": 1650
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 8.834212303161621,
      "learning_rate": 3.8673333333333335e-05,
      "loss": 3.8672,
      "step": 1700
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 11.03819751739502,
      "learning_rate": 3.834e-05,
      "loss": 3.8464,
      "step": 1750
    },
    {
      "epoch": 1.2,
      "grad_norm": 9.219710350036621,
      "learning_rate": 3.800666666666667e-05,
      "loss": 3.9311,
      "step": 1800
    },
    {
      "epoch": 1.2333333333333334,
      "grad_norm": 9.927608489990234,
      "learning_rate": 3.767333333333333e-05,
      "loss": 3.9314,
      "step": 1850
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 9.212074279785156,
      "learning_rate": 3.7340000000000005e-05,
      "loss": 3.8419,
      "step": 1900
    },
    {
      "epoch": 1.3,
      "grad_norm": 9.026063919067383,
      "learning_rate": 3.7006666666666666e-05,
      "loss": 3.7853,
      "step": 1950
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 9.453701972961426,
      "learning_rate": 3.667333333333334e-05,
      "loss": 3.8803,
      "step": 2000
    },
    {
      "epoch": 1.3666666666666667,
      "grad_norm": 9.292325019836426,
      "learning_rate": 3.634e-05,
      "loss": 3.8153,
      "step": 2050
    },
    {
      "epoch": 1.4,
      "grad_norm": 10.784677505493164,
      "learning_rate": 3.600666666666667e-05,
      "loss": 3.7829,
      "step": 2100
    },
    {
      "epoch": 1.4333333333333333,
      "grad_norm": 9.646671295166016,
      "learning_rate": 3.5673333333333336e-05,
      "loss": 3.8132,
      "step": 2150
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 9.776422500610352,
      "learning_rate": 3.5340000000000004e-05,
      "loss": 3.8075,
      "step": 2200
    },
    {
      "epoch": 1.5,
      "grad_norm": 9.627577781677246,
      "learning_rate": 3.500666666666667e-05,
      "loss": 3.8927,
      "step": 2250
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 10.151811599731445,
      "learning_rate": 3.467333333333333e-05,
      "loss": 3.8327,
      "step": 2300
    },
    {
      "epoch": 1.5666666666666667,
      "grad_norm": 8.396016120910645,
      "learning_rate": 3.434e-05,
      "loss": 3.8764,
      "step": 2350
    },
    {
      "epoch": 1.6,
      "grad_norm": 8.74840259552002,
      "learning_rate": 3.400666666666667e-05,
      "loss": 3.7617,
      "step": 2400
    },
    {
      "epoch": 1.6333333333333333,
      "grad_norm": 10.037271499633789,
      "learning_rate": 3.3673333333333335e-05,
      "loss": 3.8333,
      "step": 2450
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 9.97957992553711,
      "learning_rate": 3.3339999999999996e-05,
      "loss": 3.9079,
      "step": 2500
    },
    {
      "epoch": 1.7,
      "grad_norm": 9.920883178710938,
      "learning_rate": 3.300666666666667e-05,
      "loss": 3.7656,
      "step": 2550
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 13.614631652832031,
      "learning_rate": 3.267333333333333e-05,
      "loss": 3.8194,
      "step": 2600
    },
    {
      "epoch": 1.7666666666666666,
      "grad_norm": 9.690211296081543,
      "learning_rate": 3.2340000000000005e-05,
      "loss": 3.8434,
      "step": 2650
    },
    {
      "epoch": 1.8,
      "grad_norm": 10.903257369995117,
      "learning_rate": 3.2006666666666666e-05,
      "loss": 3.82,
      "step": 2700
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 10.100296974182129,
      "learning_rate": 3.1673333333333334e-05,
      "loss": 3.811,
      "step": 2750
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 8.56875228881836,
      "learning_rate": 3.134e-05,
      "loss": 3.7423,
      "step": 2800
    },
    {
      "epoch": 1.9,
      "grad_norm": 9.020514488220215,
      "learning_rate": 3.100666666666667e-05,
      "loss": 3.7738,
      "step": 2850
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 10.861175537109375,
      "learning_rate": 3.067333333333334e-05,
      "loss": 3.752,
      "step": 2900
    },
    {
      "epoch": 1.9666666666666668,
      "grad_norm": 8.62070083618164,
      "learning_rate": 3.034e-05,
      "loss": 3.8413,
      "step": 2950
    },
    {
      "epoch": 2.0,
      "grad_norm": 10.792156219482422,
      "learning_rate": 3.0006666666666665e-05,
      "loss": 3.6952,
      "step": 3000
    },
    {
      "epoch": 2.033333333333333,
      "grad_norm": 9.496901512145996,
      "learning_rate": 2.9673333333333336e-05,
      "loss": 3.7838,
      "step": 3050
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 10.357809066772461,
      "learning_rate": 2.934e-05,
      "loss": 3.7532,
      "step": 3100
    },
    {
      "epoch": 2.1,
      "grad_norm": 9.386348724365234,
      "learning_rate": 2.9006666666666665e-05,
      "loss": 3.7643,
      "step": 3150
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 8.924216270446777,
      "learning_rate": 2.8673333333333336e-05,
      "loss": 3.7072,
      "step": 3200
    },
    {
      "epoch": 2.1666666666666665,
      "grad_norm": 11.719286918640137,
      "learning_rate": 2.834e-05,
      "loss": 3.5959,
      "step": 3250
    },
    {
      "epoch": 2.2,
      "grad_norm": 8.82252311706543,
      "learning_rate": 2.800666666666667e-05,
      "loss": 3.8695,
      "step": 3300
    },
    {
      "epoch": 2.2333333333333334,
      "grad_norm": 9.858663558959961,
      "learning_rate": 2.7673333333333335e-05,
      "loss": 3.7241,
      "step": 3350
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 8.130671501159668,
      "learning_rate": 2.734e-05,
      "loss": 3.8119,
      "step": 3400
    },
    {
      "epoch": 2.3,
      "grad_norm": 10.664406776428223,
      "learning_rate": 2.700666666666667e-05,
      "loss": 3.6946,
      "step": 3450
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 10.028328895568848,
      "learning_rate": 2.6673333333333334e-05,
      "loss": 3.6684,
      "step": 3500
    },
    {
      "epoch": 2.3666666666666667,
      "grad_norm": 11.277518272399902,
      "learning_rate": 2.6340000000000002e-05,
      "loss": 3.7871,
      "step": 3550
    },
    {
      "epoch": 2.4,
      "grad_norm": 8.576766014099121,
      "learning_rate": 2.600666666666667e-05,
      "loss": 3.6503,
      "step": 3600
    },
    {
      "epoch": 2.4333333333333336,
      "grad_norm": 9.912038803100586,
      "learning_rate": 2.5673333333333334e-05,
      "loss": 3.8365,
      "step": 3650
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 9.477389335632324,
      "learning_rate": 2.534e-05,
      "loss": 3.6697,
      "step": 3700
    },
    {
      "epoch": 2.5,
      "grad_norm": 11.110730171203613,
      "learning_rate": 2.5006666666666666e-05,
      "loss": 3.6844,
      "step": 3750
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 8.901559829711914,
      "learning_rate": 2.4673333333333333e-05,
      "loss": 3.7678,
      "step": 3800
    },
    {
      "epoch": 2.5666666666666664,
      "grad_norm": 10.884561538696289,
      "learning_rate": 2.434e-05,
      "loss": 3.7985,
      "step": 3850
    },
    {
      "epoch": 2.6,
      "grad_norm": 8.95080852508545,
      "learning_rate": 2.400666666666667e-05,
      "loss": 3.6868,
      "step": 3900
    },
    {
      "epoch": 2.6333333333333333,
      "grad_norm": 11.090102195739746,
      "learning_rate": 2.3673333333333333e-05,
      "loss": 3.7743,
      "step": 3950
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 10.262835502624512,
      "learning_rate": 2.334e-05,
      "loss": 3.7252,
      "step": 4000
    },
    {
      "epoch": 2.7,
      "grad_norm": 12.198770523071289,
      "learning_rate": 2.3006666666666668e-05,
      "loss": 3.7095,
      "step": 4050
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 11.85738468170166,
      "learning_rate": 2.2673333333333335e-05,
      "loss": 3.7677,
      "step": 4100
    },
    {
      "epoch": 2.7666666666666666,
      "grad_norm": 10.274803161621094,
      "learning_rate": 2.234e-05,
      "loss": 3.6909,
      "step": 4150
    },
    {
      "epoch": 2.8,
      "grad_norm": 9.949324607849121,
      "learning_rate": 2.2006666666666667e-05,
      "loss": 3.7339,
      "step": 4200
    },
    {
      "epoch": 2.8333333333333335,
      "grad_norm": 12.836930274963379,
      "learning_rate": 2.1673333333333335e-05,
      "loss": 3.623,
      "step": 4250
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 9.40255069732666,
      "learning_rate": 2.1340000000000002e-05,
      "loss": 3.7236,
      "step": 4300
    },
    {
      "epoch": 2.9,
      "grad_norm": 9.157392501831055,
      "learning_rate": 2.100666666666667e-05,
      "loss": 3.7267,
      "step": 4350
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 10.335104942321777,
      "learning_rate": 2.0673333333333334e-05,
      "loss": 3.7705,
      "step": 4400
    },
    {
      "epoch": 2.966666666666667,
      "grad_norm": 9.858963012695312,
      "learning_rate": 2.0340000000000002e-05,
      "loss": 3.7368,
      "step": 4450
    },
    {
      "epoch": 3.0,
      "grad_norm": 9.6728515625,
      "learning_rate": 2.000666666666667e-05,
      "loss": 3.6795,
      "step": 4500
    },
    {
      "epoch": 3.033333333333333,
      "grad_norm": 9.380072593688965,
      "learning_rate": 1.9673333333333337e-05,
      "loss": 3.6022,
      "step": 4550
    },
    {
      "epoch": 3.066666666666667,
      "grad_norm": 11.564696311950684,
      "learning_rate": 1.934e-05,
      "loss": 3.6593,
      "step": 4600
    },
    {
      "epoch": 3.1,
      "grad_norm": 10.634299278259277,
      "learning_rate": 1.9006666666666665e-05,
      "loss": 3.6278,
      "step": 4650
    },
    {
      "epoch": 3.1333333333333333,
      "grad_norm": 9.798027992248535,
      "learning_rate": 1.8673333333333333e-05,
      "loss": 3.6801,
      "step": 4700
    },
    {
      "epoch": 3.1666666666666665,
      "grad_norm": 10.620755195617676,
      "learning_rate": 1.834e-05,
      "loss": 3.6279,
      "step": 4750
    },
    {
      "epoch": 3.2,
      "grad_norm": 9.840755462646484,
      "learning_rate": 1.8006666666666668e-05,
      "loss": 3.6475,
      "step": 4800
    },
    {
      "epoch": 3.2333333333333334,
      "grad_norm": 10.474050521850586,
      "learning_rate": 1.7673333333333332e-05,
      "loss": 3.795,
      "step": 4850
    },
    {
      "epoch": 3.2666666666666666,
      "grad_norm": 9.900246620178223,
      "learning_rate": 1.734e-05,
      "loss": 3.7491,
      "step": 4900
    },
    {
      "epoch": 3.3,
      "grad_norm": 10.235739707946777,
      "learning_rate": 1.7006666666666668e-05,
      "loss": 3.7507,
      "step": 4950
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 9.18857192993164,
      "learning_rate": 1.6673333333333335e-05,
      "loss": 3.5786,
      "step": 5000
    },
    {
      "epoch": 3.3666666666666667,
      "grad_norm": 9.707728385925293,
      "learning_rate": 1.634e-05,
      "loss": 3.7151,
      "step": 5050
    },
    {
      "epoch": 3.4,
      "grad_norm": 9.995055198669434,
      "learning_rate": 1.6006666666666667e-05,
      "loss": 3.6712,
      "step": 5100
    },
    {
      "epoch": 3.4333333333333336,
      "grad_norm": 10.024064064025879,
      "learning_rate": 1.5673333333333335e-05,
      "loss": 3.6635,
      "step": 5150
    },
    {
      "epoch": 3.466666666666667,
      "grad_norm": 9.536625862121582,
      "learning_rate": 1.5340000000000002e-05,
      "loss": 3.7025,
      "step": 5200
    },
    {
      "epoch": 3.5,
      "grad_norm": 8.929551124572754,
      "learning_rate": 1.5006666666666666e-05,
      "loss": 3.6021,
      "step": 5250
    },
    {
      "epoch": 3.533333333333333,
      "grad_norm": 8.42786979675293,
      "learning_rate": 1.4673333333333334e-05,
      "loss": 3.6368,
      "step": 5300
    },
    {
      "epoch": 3.5666666666666664,
      "grad_norm": 9.290170669555664,
      "learning_rate": 1.434e-05,
      "loss": 3.6311,
      "step": 5350
    },
    {
      "epoch": 3.6,
      "grad_norm": 9.812708854675293,
      "learning_rate": 1.4006666666666668e-05,
      "loss": 3.681,
      "step": 5400
    },
    {
      "epoch": 3.6333333333333333,
      "grad_norm": 9.34447956085205,
      "learning_rate": 1.3673333333333335e-05,
      "loss": 3.6563,
      "step": 5450
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 10.053263664245605,
      "learning_rate": 1.334e-05,
      "loss": 3.6421,
      "step": 5500
    },
    {
      "epoch": 3.7,
      "grad_norm": 9.303692817687988,
      "learning_rate": 1.3006666666666667e-05,
      "loss": 3.735,
      "step": 5550
    },
    {
      "epoch": 3.7333333333333334,
      "grad_norm": 10.837058067321777,
      "learning_rate": 1.2673333333333335e-05,
      "loss": 3.693,
      "step": 5600
    },
    {
      "epoch": 3.7666666666666666,
      "grad_norm": 10.31525993347168,
      "learning_rate": 1.234e-05,
      "loss": 3.6496,
      "step": 5650
    },
    {
      "epoch": 3.8,
      "grad_norm": 10.550507545471191,
      "learning_rate": 1.2006666666666668e-05,
      "loss": 3.606,
      "step": 5700
    },
    {
      "epoch": 3.8333333333333335,
      "grad_norm": 10.308088302612305,
      "learning_rate": 1.1673333333333334e-05,
      "loss": 3.7148,
      "step": 5750
    },
    {
      "epoch": 3.8666666666666667,
      "grad_norm": 10.101726531982422,
      "learning_rate": 1.134e-05,
      "loss": 3.5701,
      "step": 5800
    },
    {
      "epoch": 3.9,
      "grad_norm": 9.827783584594727,
      "learning_rate": 1.1006666666666666e-05,
      "loss": 3.6616,
      "step": 5850
    },
    {
      "epoch": 3.9333333333333336,
      "grad_norm": 10.218140602111816,
      "learning_rate": 1.0673333333333333e-05,
      "loss": 3.7166,
      "step": 5900
    },
    {
      "epoch": 3.966666666666667,
      "grad_norm": 9.122310638427734,
      "learning_rate": 1.0340000000000001e-05,
      "loss": 3.7655,
      "step": 5950
    },
    {
      "epoch": 4.0,
      "grad_norm": 10.22310733795166,
      "learning_rate": 1.0006666666666667e-05,
      "loss": 3.6526,
      "step": 6000
    },
    {
      "epoch": 4.033333333333333,
      "grad_norm": 11.085783004760742,
      "learning_rate": 9.673333333333334e-06,
      "loss": 3.5968,
      "step": 6050
    },
    {
      "epoch": 4.066666666666666,
      "grad_norm": 9.509137153625488,
      "learning_rate": 9.34e-06,
      "loss": 3.6444,
      "step": 6100
    },
    {
      "epoch": 4.1,
      "grad_norm": 10.858405113220215,
      "learning_rate": 9.006666666666668e-06,
      "loss": 3.631,
      "step": 6150
    },
    {
      "epoch": 4.133333333333334,
      "grad_norm": 10.213476181030273,
      "learning_rate": 8.673333333333334e-06,
      "loss": 3.5973,
      "step": 6200
    },
    {
      "epoch": 4.166666666666667,
      "grad_norm": 11.027027130126953,
      "learning_rate": 8.34e-06,
      "loss": 3.7074,
      "step": 6250
    },
    {
      "epoch": 4.2,
      "grad_norm": 9.863179206848145,
      "learning_rate": 8.006666666666666e-06,
      "loss": 3.7623,
      "step": 6300
    },
    {
      "epoch": 4.233333333333333,
      "grad_norm": 9.42431354522705,
      "learning_rate": 7.673333333333333e-06,
      "loss": 3.6249,
      "step": 6350
    },
    {
      "epoch": 4.266666666666667,
      "grad_norm": 9.57085132598877,
      "learning_rate": 7.340000000000001e-06,
      "loss": 3.5999,
      "step": 6400
    },
    {
      "epoch": 4.3,
      "grad_norm": 9.906200408935547,
      "learning_rate": 7.006666666666667e-06,
      "loss": 3.6891,
      "step": 6450
    },
    {
      "epoch": 4.333333333333333,
      "grad_norm": 9.70141315460205,
      "learning_rate": 6.673333333333334e-06,
      "loss": 3.6703,
      "step": 6500
    },
    {
      "epoch": 4.366666666666666,
      "grad_norm": 10.802477836608887,
      "learning_rate": 6.34e-06,
      "loss": 3.6755,
      "step": 6550
    },
    {
      "epoch": 4.4,
      "grad_norm": 8.342082023620605,
      "learning_rate": 6.006666666666667e-06,
      "loss": 3.6922,
      "step": 6600
    },
    {
      "epoch": 4.433333333333334,
      "grad_norm": 10.13381576538086,
      "learning_rate": 5.673333333333333e-06,
      "loss": 3.5955,
      "step": 6650
    },
    {
      "epoch": 4.466666666666667,
      "grad_norm": 9.303979873657227,
      "learning_rate": 5.3400000000000005e-06,
      "loss": 3.5291,
      "step": 6700
    },
    {
      "epoch": 4.5,
      "grad_norm": 11.002058029174805,
      "learning_rate": 5.006666666666667e-06,
      "loss": 3.66,
      "step": 6750
    },
    {
      "epoch": 4.533333333333333,
      "grad_norm": 11.921427726745605,
      "learning_rate": 4.673333333333334e-06,
      "loss": 3.656,
      "step": 6800
    },
    {
      "epoch": 4.566666666666666,
      "grad_norm": 11.165121078491211,
      "learning_rate": 4.34e-06,
      "loss": 3.5898,
      "step": 6850
    },
    {
      "epoch": 4.6,
      "grad_norm": 8.81485652923584,
      "learning_rate": 4.006666666666667e-06,
      "loss": 3.6513,
      "step": 6900
    },
    {
      "epoch": 4.633333333333333,
      "grad_norm": 10.310001373291016,
      "learning_rate": 3.6733333333333335e-06,
      "loss": 3.6312,
      "step": 6950
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 9.303387641906738,
      "learning_rate": 3.34e-06,
      "loss": 3.6453,
      "step": 7000
    },
    {
      "epoch": 4.7,
      "grad_norm": 11.825582504272461,
      "learning_rate": 3.0066666666666665e-06,
      "loss": 3.6579,
      "step": 7050
    },
    {
      "epoch": 4.733333333333333,
      "grad_norm": 9.385102272033691,
      "learning_rate": 2.6733333333333337e-06,
      "loss": 3.5577,
      "step": 7100
    },
    {
      "epoch": 4.766666666666667,
      "grad_norm": 9.857261657714844,
      "learning_rate": 2.34e-06,
      "loss": 3.5695,
      "step": 7150
    },
    {
      "epoch": 4.8,
      "grad_norm": 11.031064987182617,
      "learning_rate": 2.006666666666667e-06,
      "loss": 3.5752,
      "step": 7200
    },
    {
      "epoch": 4.833333333333333,
      "grad_norm": 10.068244934082031,
      "learning_rate": 1.6733333333333333e-06,
      "loss": 3.575,
      "step": 7250
    },
    {
      "epoch": 4.866666666666667,
      "grad_norm": 9.968668937683105,
      "learning_rate": 1.34e-06,
      "loss": 3.6928,
      "step": 7300
    },
    {
      "epoch": 4.9,
      "grad_norm": 11.886947631835938,
      "learning_rate": 1.0066666666666666e-06,
      "loss": 3.6583,
      "step": 7350
    },
    {
      "epoch": 4.933333333333334,
      "grad_norm": 11.260744094848633,
      "learning_rate": 6.733333333333334e-07,
      "loss": 3.6894,
      "step": 7400
    },
    {
      "epoch": 4.966666666666667,
      "grad_norm": 9.63245964050293,
      "learning_rate": 3.4e-07,
      "loss": 3.6375,
      "step": 7450
    },
    {
      "epoch": 5.0,
      "grad_norm": 10.567501068115234,
      "learning_rate": 6.666666666666668e-09,
      "loss": 3.6149,
      "step": 7500
    }
  ],
  "logging_steps": 50,
  "max_steps": 7500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 400,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 6.0799866236928e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
