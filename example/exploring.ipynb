{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4a631f",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install transformers accelerate datasets pillow peft bitsandbytes torchvision torchaudio gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eafdce6",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in c:\\users\\nirva\\onedrive\\projects\\multimodal\\multimodal-exploring\\.venv\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: kaggle in c:\\users\\nirva\\onedrive\\projects\\multimodal\\multimodal-exploring\\.venv\\lib\\site-packages (1.7.4.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\nirva\\onedrive\\projects\\multimodal\\multimodal-exploring\\.venv\\lib\\site-packages (from kagglehub) (25.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\nirva\\onedrive\\projects\\multimodal\\multimodal-exploring\\.venv\\lib\\site-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\nirva\\onedrive\\projects\\multimodal\\multimodal-exploring\\.venv\\lib\\site-packages (from kagglehub) (2.32.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nirva\\onedrive\\projects\\multimodal\\multimodal-exploring\\.venv\\lib\\site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: bleach in c:\\users\\nirva\\onedrive\\projects\\multimodal\\multimodal-exploring\\.venv\\lib\\site-packages (from kaggle) (6.2.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\users\\nirva\\onedrive\\projects\\multimodal\\multimodal-exploring\\.venv\\lib\\site-packages (from kaggle) (2025.8.3)\n",
      "Requirement already satisfied: charset-normalizer in c:\\users\\nirva\\onedrive\\projects\\multimodal\\multimodal-exploring\\.venv\\lib\\site-packages (from kaggle) (3.4.3)\n",
      "Requirement already satisfied: idna in c:\\users\\nirva\\onedrive\\projects\\multimodal\\multimodal-exploring\\.venv\\lib\\site-packages (from kaggle) (3.10)\n",
      "Requirement already satisfied: protobuf in c:\\users\\nirva\\onedrive\\projects\\multimodal\\multimodal-exploring\\.venv\\lib\\site-packages (from kaggle) (6.32.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\nirva\\onedrive\\projects\\multimodal\\multimodal-exploring\\.venv\\lib\\site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\nirva\\onedrive\\projects\\multimodal\\multimodal-exploring\\.venv\\lib\\site-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in c:\\users\\nirva\\onedrive\\projects\\multimodal\\multimodal-exploring\\.venv\\lib\\site-packages (from kaggle) (65.5.0)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\nirva\\onedrive\\projects\\multimodal\\multimodal-exploring\\.venv\\lib\\site-packages (from kaggle) (1.17.0)\n",
      "Requirement already satisfied: text-unidecode in c:\\users\\nirva\\onedrive\\projects\\multimodal\\multimodal-exploring\\.venv\\lib\\site-packages (from kaggle) (1.3)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in c:\\users\\nirva\\onedrive\\projects\\multimodal\\multimodal-exploring\\.venv\\lib\\site-packages (from kaggle) (2.5.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\nirva\\onedrive\\projects\\multimodal\\multimodal-exploring\\.venv\\lib\\site-packages (from kaggle) (0.5.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\nirva\\onedrive\\projects\\multimodal\\multimodal-exploring\\.venv\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66ec8b4e",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/adityajn105/flickr8k\n",
      "Dataset downloaded and extracted to: None\n"
     ]
    }
   ],
   "source": [
    "import kaggle\n",
    "\n",
    "# Download dataset from Kaggle\n",
    "\n",
    "path = kaggle.api.dataset_download_files(\"adityajn105/flickr8k\", unzip=True, path=\"./data\")\n",
    "\n",
    "print(f\"Dataset downloaded and extracted to: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ee6d11",
   "metadata": {},
   "source": [
    "Below is just imports and setting threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e63211d5",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import os, json, random, time, math\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "\n",
    "intensity = 5  # Scale of 1-10 for CPU usage\n",
    "torch.set_num_threads(max(1, int(os.cpu_count() // (1 / (intensity / 10))))) # Sets the number of threads to the cpu_count divided by the reciprocal of the intensity(scaled 1 - 10) divided by 10\n",
    "\n",
    "from PIL import Image\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "\n",
    "    AutoTokenizer,\n",
    "    AutoProcessor,\n",
    "    BlipForConditionalGeneration,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    default_data_collator,\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7249990",
   "metadata": {},
   "source": [
    "Ok, so basically I am on CPU, so I'll put a snippet of code below that reduces the size of the dataset and processes it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7745338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data\\Images\\1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data\\Images\\1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data\\Images\\1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data\\Images\\1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data\\Images\\1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   image  \\\n",
       "0  data\\Images\\1000268201_693b08cb0e.jpg   \n",
       "1  data\\Images\\1000268201_693b08cb0e.jpg   \n",
       "2  data\\Images\\1000268201_693b08cb0e.jpg   \n",
       "3  data\\Images\\1000268201_693b08cb0e.jpg   \n",
       "4  data\\Images\\1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                             caption  \n",
       "0  A child in a pink dress is climbing up a set o...  \n",
       "1              A girl going into a wooden building .  \n",
       "2   A little girl climbing into a wooden playhouse .  \n",
       "3  A little girl climbing the stairs to her playh...  \n",
       "4  A little girl in a pink dress going into a woo...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Paths -> Data, Images, Captions.txt; Replace your paths here\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "IMAGES_DIR = DATA_DIR / \"Images\"\n",
    "CAPTIONS_FILE = DATA_DIR / \"captions.txt\"\n",
    "\n",
    "# Make sure nothing is missing, everything exists, etc.\n",
    "\n",
    "assert IMAGES_DIR.exists(), f\"Images directory {IMAGES_DIR} does not exist. Run the cells above.\"\n",
    "assert CAPTIONS_FILE.exists(), f\"Captions file {CAPTIONS_FILE} does not exist. Run the cells above.\"\n",
    "\n",
    "# Function: \n",
    "\n",
    "def parse_captions(captions_file, images_dir):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    Takes in two arguments:\n",
    "\n",
    "    - captions_file: The path to the captions file\n",
    "    - images_dir: The path to the directory containing images\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    - df: A pandas DataFrame with two columns: \"image\" and \"caption\"\n",
    "    - caps_by_image: A dictionary mapping each image filename to a list of its captions\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "\n",
    "        df = pd.read_csv(captions_file)\n",
    "\n",
    "        if set(df.columns.map(str.lower)) >= {\"image\", \"caption\"}:\n",
    "\n",
    "            df = df.rename(columns={i: i.lower() for i in df.columns})\n",
    "\n",
    "        else:\n",
    "\n",
    "            df = pd.read_csv(captions_file, header=None, names=[\"image\", \"caption\"])\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Error reading {captions_file}: {e}\")\n",
    "\n",
    "        df = pd.read_csv(captions_file, header=None, names=[\"image\", \"caption\"], engine=\"python\")\n",
    "\n",
    "    # Cleaning: Remove whitespaces. Check rows for missing files and remove both that image and caption\n",
    "\n",
    "    df[\"image\"] = df[\"image\"].astype(str).str.strip()\n",
    "    df[\"caption\"] = df[\"caption\"].astype(str).str.strip()\n",
    "    df = df[\"image\"].map(lambda yo: (images_dir / yo)).to_frame().join(df[\"caption\"])\n",
    "\n",
    "    # Group captions that belong to the same image\n",
    "    # We will, however, only use one caption per image for training\n",
    "\n",
    "    caps_by_img = defaultdict(list)\n",
    "\n",
    "    for r in df.itertuples(index=False):\n",
    "\n",
    "        caps_by_img[r.image].append(r.caption)\n",
    "\n",
    "    return df, caps_by_img\n",
    "\n",
    "df, caps_by_img = parse_captions(CAPTIONS_FILE, IMAGES_DIR)\n",
    "\n",
    "df.head()  # First 5 rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01e427a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 1200, 800)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Split\n",
    "\n",
    "pairs = [\n",
    "    \n",
    "    {\"image_path\": str(IMAGES_DIR / f), \"caption\": caps[0]}\n",
    "\n",
    "         for f, caps in caps_by_img.items()\n",
    "         \n",
    "         ]\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(pairs)\n",
    "\n",
    "split = [.75, .15, .1]  # Train, Val, Test\n",
    "amount = 8000\n",
    "\n",
    "n = len(pairs)\n",
    "n_train = min(amount * split[0], int(split[0] * n))\n",
    "n_val = min(amount * split[1], int(split[1] * n))\n",
    "\n",
    "train_pairs = pairs[: int(n_train)]\n",
    "val_pairs = pairs[int(n_train) : int(n_train + n_val)]\n",
    "test_pairs = pairs[int(n_train + n_val) : amount]\n",
    "\n",
    "len(train_pairs), len(val_pairs), len(test_pairs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b843d53",
   "metadata": {},
   "source": [
    "Wrap it as a HuggingFace ðŸ¤— Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dfd65e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DatasetDict(\n",
    "    \n",
    "    {\n",
    "        \n",
    "        \"train\": Dataset.from_pandas(pd.DataFrame(train_pairs)),\n",
    "        \"val\": Dataset.from_pandas(pd.DataFrame(val_pairs)),\n",
    "        \"test\": Dataset.from_pandas(pd.DataFrame(test_pairs)),\n",
    "\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084acfc4",
   "metadata": {},
   "source": [
    "Load BLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "00fe95d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "cap_id = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "\n",
    "image_processor = ViTImageProcessor.from_pretrained(cap_id)\n",
    "tokenizer       = AutoTokenizer.from_pretrained(cap_id)\n",
    "model           = VisionEncoderDecoderModel.from_pretrained(cap_id).to(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600a346b",
   "metadata": {},
   "source": [
    "Time to preprocess !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9a576e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(batch):\n",
    "\n",
    "    images = []\n",
    "\n",
    "    for image_path in batch[\"image_path\"]:\n",
    "\n",
    "        try:\n",
    "\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        except Exception as e:\n",
    "\n",
    "            img = Image.new(\"RGB\", (2,2), color = (0, 0, 0))\n",
    "\n",
    "        images.append(img)\n",
    "    \n",
    "    pix = image_processor(\n",
    "        \n",
    "        images=images, \n",
    "        \n",
    "        return_tensors=\"pt\"\n",
    "        \n",
    "        )\n",
    "\n",
    "    tok = tokenizer(\n",
    "        \n",
    "        batch[\"caption\"], \n",
    "        \n",
    "        padding=\"max_length\", \n",
    "        \n",
    "        max_length=32, \n",
    "        \n",
    "        return_tensors=\"pt\"\n",
    "        \n",
    "        )\n",
    "\n",
    "    labels = tok.input_ids.clone()\n",
    "\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "    return {\"pixel_values\": pix[\"pixel_values\"], \"labels\": labels}\n",
    "\n",
    "\n",
    "processed_ds = ds.with_transform(preprocess)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a40e8a",
   "metadata": {},
   "source": [
    "TIME TO TRAIN !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5c9911c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nirva\\OneDrive\\Projects\\multimodal\\multimodal-exploring\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   3/1500 00:16 < 6:39:39, 0.06 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m      3\u001b[39m args = TrainingArguments(\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m     output_dir=OUT_DIR,\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m )\n\u001b[32m     23\u001b[39m trainer = Trainer(\n\u001b[32m     24\u001b[39m \n\u001b[32m     25\u001b[39m     model=model,\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m     data_collator=default_data_collator,\n\u001b[32m     30\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m train_res = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m trainer.save_model(OUT_DIR)\n\u001b[32m     34\u001b[39m processor.save_pretrained(OUT_DIR)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nirva\\OneDrive\\Projects\\multimodal\\multimodal-exploring\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2328\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2327\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nirva\\OneDrive\\Projects\\multimodal\\multimodal-exploring\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2672\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2665\u001b[39m context = (\n\u001b[32m   2666\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2667\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2668\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2670\u001b[39m )\n\u001b[32m   2671\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2672\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2674\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2675\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2676\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2677\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2678\u001b[39m ):\n\u001b[32m   2679\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2680\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nirva\\OneDrive\\Projects\\multimodal\\multimodal-exploring\\.venv\\Lib\\site-packages\\transformers\\trainer.py:4060\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   4057\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   4058\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4060\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4062\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nirva\\OneDrive\\Projects\\multimodal\\multimodal-exploring\\.venv\\Lib\\site-packages\\accelerate\\accelerator.py:2734\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2732\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2733\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2734\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nirva\\OneDrive\\Projects\\multimodal\\multimodal-exploring\\.venv\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nirva\\OneDrive\\Projects\\multimodal\\multimodal-exploring\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nirva\\OneDrive\\Projects\\multimodal\\multimodal-exploring\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "OUT_DIR = \"finetuned-model-blip-flicker8k\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "\n",
    "    output_dir=OUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    eval_steps=200,\n",
    "    logging_steps=50,\n",
    "    save_steps=400,\n",
    "    save_total_limit=1,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=0,   #\n",
    "    report_to=[],\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=processed_ds[\"train\"],\n",
    "    eval_dataset=processed_ds[\"val\"],\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "\n",
    "train_res = trainer.train()\n",
    "trainer.save_model(OUT_DIR)\n",
    "processor.save_pretrained(OUT_DIR)\n",
    "\n",
    "train_res\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
